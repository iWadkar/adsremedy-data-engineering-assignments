Data Engineering Assignment 1
Employee Data Pipeline (Spark + PostgreSQL + Docker)

Overview:
‚Ä¢ This project implements an end-to-end employee data pipeline using Apache Spark (PySpark) and PostgreSQL, fully containerized with Docker Compose.

The pipeline demonstrates:
‚Ä¢ realistic dirty data generation
‚Ä¢ robust data cleaning and transformation using Spark
‚Ä¢ strict data integrity enforcement using PostgreSQL constraints
‚Ä¢ professional configuration and secret management using environment variables
‚Ä¢ The final output is a clean, analytics-ready employees_clean table in PostgreSQL.

Architecture:
Raw CSV (employees_raw.csv) --> Apache Spark (PySpark ETL) --> PostgreSQL (employees_clean table)
‚Ä¢ All components run locally inside Docker containers.

Tech Stack:
‚Ä¢ Apache Spark 3.5 (PySpark)
‚Ä¢ PostgreSQL 15
‚Ä¢ Docker & Docker Compose
‚Ä¢ Python (Faker for data generation)

Project Structure:
üìÅ assignment_one/
    ‚Ä¢ docker-compose.yml
    ‚Ä¢ .env.example
    ‚Ä¢ README.md
üìÅ data/
    ‚Ä¢ employees_raw.csv
üìÅ jars/
    ‚Ä¢ postgresql-42.7.3.jar
üìÅ scripts/
    ‚Ä¢ generate_employees_data.py
üìÅ spark/
    ‚Ä¢ employee_etl.py
üìÅ sql/
    ‚Ä¢ init.sql

Data Generation:
‚Ä¢ A Python script generates 1200+ employee records with intentional data quality issues:

Included issues:
‚Ä¢ duplicate employee_id
‚Ä¢ duplicate and invalid emails
‚Ä¢ future hire_date
‚Ä¢ salaries with currency symbols and commas
‚Ä¢ mixed case text
‚Ä¢ null values in non-critical fields

Script:
`python scripts/generate_employees_data.py`

Output:
`data/employees_raw.csv`

Environment Configuration:
‚Ä¢ Sensitive configuration is handled via environment variables.

.env.example (committed)
`DB_HOST=postgres
DB_PORT=5432
DB_NAME=employees
DB_USER=admin
DB_PASSWORD=admin
SPARK_APP_NAME=EmployeeETL`

Setup:
cp .env.example .env

‚Ä¢ The .env file is intentionally not committed to version control.

Database Schema:
‚Ä¢ PostgreSQL table is created automatically on first startup:

CREATE TABLE employees_clean (
    employee_id INTEGER PRIMARY KEY,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    full_name VARCHAR(100) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    email_domain VARCHAR(50),
    hire_date DATE NOT NULL,
    job_title VARCHAR(100),
    department VARCHAR(50),
    salary DECIMAL(10,2),
    salary_band VARCHAR(20),
    manager_id INTEGER,
    address TEXT,
    city VARCHAR(50),
    state VARCHAR(2),
    zip_code VARCHAR(10),
    birth_date DATE,
    age INTEGER,
    tenure_years DECIMAL(3,1),
    status VARCHAR(20) DEFAULT 'Active',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

‚Ä¢ The database enforces NOT NULL, PRIMARY KEY, and UNIQUE constraints.


Spark ETL Logic:-

The PySpark job performs:
Data Quality Handling
‚Ä¢ drops rows with null critical fields
‚Ä¢ removes duplicate employee_id
‚Ä¢ validates email format
‚Ä¢ removes future hire_date
‚Ä¢ enforces email uniqueness (to satisfy DB constraint)

Transformations
‚Ä¢ name standardization
‚Ä¢ salary cleaning and casting
‚Ä¢ salary band classification
‚Ä¢ age and tenure calculation

Enrichment
‚Ä¢ full_name
‚Ä¢ email_domain

Design Choice:
‚Ä¢ Invalid records are intentionally dropped rather than defaulting to preserve data integrity.

Running the Pipeline:
1. Start services
docker compose up -d

2. Run Spark ETL
docker exec -it spark /opt/spark/bin/spark-submit --jars /opt/jars/postgresql-42.7.7.jar --driver-class-path /opt/jars/postgresql-42.7.7.jar /opt/spark-apps/employee_etl.py

3. Verification:-
‚Ä¢ Connect to PostgreSQL:
docker exec -it postgres_db psql -U admin -d employees

‚Ä¢ Run:
SELECT COUNT(*) FROM employees_clean;

SELECT employee_id, full_name, salary
FROM employees_clean
LIMIT 5;

Expected outcome:
‚Ä¢ Final row count < raw count (by design)
‚Ä¢ No nulls in critical columns
‚Ä¢ Clean, query-ready data

4. Notes on Data Loss:-
The reduction from raw records to final records is intentional and due to:
‚Ä¢ invalid emails
‚Ä¢ duplicate business keys
‚Ä¢ future hire dates
‚Ä¢ null critical fields
This demonstrates defensive data engineering, not pipeline failure.

5. Sample Data:-
‚Ä¢ The repository includes a sample raw input file (employees_raw.csv) and a sample cleaned output (employees_clean_sample.csv) as required by the assignment.
‚Ä¢ These files are representative examples; the pipeline can regenerate data using the provided scripts.

6. How to Re-run Cleanly
‚Ä¢ To restart containers (data preserved):
docker compose down
docker compose up -d

‚Ä¢ To reset everything including data:
docker compose down -v

## Demo Video
A short demo video showing environment setup, Spark ETL execution, and PostgreSQL verification:
[Demo video](https://drive.google.com/file/d/1akP4JvsKNhmSdPmXQubn3Yd3H8Z08gJy/view?usp=sharing)
